<!DOCTYPE HTML>
<!--
    Astral by HTML5 UP
    html5up.net | @ajlkn
    Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<!--
    Edited by Bruno Beltran for personal use
    brunobeltran.com | brunobeltran0 AT gmail.com
    Free for personal and commercial use under the CCA 3.0 license.
-->
<html>
  <head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-29334348-1"></script>
    <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());

    gtag('config', 'UA-29334348-1');
    </script>

    <title>Bruno Beltran | Personal Site</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <meta name="keywords" content="bruno,beltran,stanford,biophysics,math,python,philosophy,blog">
    <meta name="author" content="Bruno Beltran">
    <meta name="description" content="A biophysics Ph.D. student at Stanford University.">
    <meta property="og:image" content="images/me.jpg">
    <link rel="stylesheet" href="assets/css/main.css" />
    <!-- Favicon links -->
      <link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
      <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
      <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <!-- Configuration for saving to homescreen as bookmark -->
      <link rel="manifest" href="/site.webmanifest">
      <link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
      <meta name="msapplication-TileColor" content="#da532c">
      <meta name="theme-color" content="#ffffff">
      <script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
      </script>
    <noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
  </head>

  <body class="is-preload">

    <!-- Wrapper-->
      <div id="wrapper">

        <!-- Nav -->
          <nav id="nav">
            <a href="#" class="icon solid fa-home"><span>Home</span></a>
            <a href="#code" class="icon solid fa-code"><span>Code</span></a>
            <a href="#science" class="icon solid fa-flask"><span>Science</span></a>
            <a href="#writing" class="icon solid fa-folder"><span>Writing</span></a>
            <a href="#contact" class="icon solid fa-envelope"><span>Contact</span></a>
          </nav>

        <!-- Main -->
          <div id="main">

            <!-- Me -->
              <article id="home" class="panel intro">
                <header>
                  <h1>Bruno Beltran</h1>
                  <p>Mathematician, Biophysicist, Programmer</p>
                  <p><a href="files/brunocv.pdf" class="icon solid
                  fa-address-card"><span style="margin-left: 0.25em;">CV</span></a></p>
                </header>
                <a href="#code" class="jumplink pic">
                  <span class="arrow icon solid fa-chevron-right"><span>See my work</span></span>
                  <img src="images/me.jpg" alt="" />
                </a>
              </article>

            <!-- Code -->
              <article id="code" class="panel">
                <header>
                  <h2>Code</h2>
                  <p>
Click the links below to view some of my more popular projects.
</p>
<p>
For access to unpublished code, please see
<a href="https://github.com/brunobeltran"> my Github</a> or <a href="#contact">
contact me</a>.
</p>

                  <span class="nav">
                    <a href="#home" class="jumplink pic">
                      <span class="arrow left icon solid fa-chevron-left"><span>Go back to home page</span></span>
                    </a>
                    <a href="#science" class="jumplink pic">
                      <span class="arrow right icon solid fa-chevron-right"><span>See my scientifit work</span></span>
                    </a>
                  </span>
                </header>
                <section>
                  <div class="row">
<!-- col-n(-size) makes a column that takes up n/12'ths of the row. -size can be -->
<!-- -small, -medium, -large, -xlarge (or none for default). these correspond to -->
<!-- max-width of 736px, 980px, 1280px, 1680px, respectively. -->
                    
                    <div class="col-4 col-6-medium col-12-small">
                      <a href="http://python-tooling-example.rtfd.io
">
                      <h4>Python Tooling How-To and Examples</h4>
                        <img class="image fit" src="./images/pte-doc-screenshot.jpg"
                        alt="Image representing Python Tooling How-To and Examples.">
                      </a>
                    </div>
                    
                    <div class="col-4 col-6-medium col-12-small">
                      <a href="http://multi-locus-analysis.rtfd.io
">
                      <h4>(Multiple) Trajectory Analysis Toolkit</h4>
                        <img class="image fit" src="./images/burgess-example-data.jpg"
                        alt="Image representing (Multiple) Trajectory Analysis Toolkit.">
                      </a>
                    </div>
                    
                    <div class="col-4 col-6-medium col-12-small">
                      <a href="https://github.com/brunobeltran/nuc_chain
">
                      <h4>Nucleosome Chain Simulation and Theory</h4>
                        <img class="image fit" src="./images/nuc-chain-diagram.jpg"
                        alt="Image representing Nucleosome Chain Simulation and Theory.">
                      </a>
                    </div>
                    
                  </div>
                </section>
              </article>

            <!-- Science -->
              <article id="science" class="panel">
                <header>
                  <h2>Science</h2>
                  <p>
<a href="https://scholar.google.com/citations?user=kL_l9GUAAAAJ&hl=en">My
    research</a> has spanned several fields, from semigroup theory to bacterial
cell size control, and from statistics to modeling chromatin organization.
</p>
<p>
Below, I highlight some recent projects:
<!--TODO: make invisble section containing this text if necessary
<br />
Under Dr. Andrew Spakowitz, I developed coarse-grained models for chromatin
that include realistic nucleosome spacing with the help of an extremely talented
undegraduate (Deepti Kannan). I studied the link between chromatin's
"compartmental organization" and epigenetic information in collaboration with
Quinn MacPherson. I contributed to multiple projects investigating the dynamics
of epigenetic mark spreading with Sarah Sandholtz. Finally, I contributed
analysis, experiments, and statistical method development to a collaboration
with the Burgess lab at UC Davis studying the mechanism of meiotic homolog
recombination.
<br />
Under Dr. Christine Jacobs-Wagner, I studied cell size control, chromosome
segregation and the effects of supercoiling on transcription (all in bacteria).
<br />
Under Drs. Castillo-Chavez and Baojun Song, I used fluid simulation to study
aneurysm formation.
<br />
Under Dr. Frank Neubrander, I studied new bounds for rational Pade approximants
to operator semigroups, with applications to Laplace inversion.
-->
</p>

                  <span class="nav">
                    <a href="#code" class="jumplink pic">
                      <span class="arrow left icon solid fa-chevron-left"><span>See my work</span></span>
                    </a>
                    <a href="#writing" class="jumplink pic">
                      <span class="arrow right icon solid fa-chevron-right"><span>Read some of my thoughts</span></span>
                    </a>
                  </span>
                </header>

                <section>
                  
                  <hr />
                  <div class="row">
<!-- col-n(-size) makes a column that takes up n/12'ths of the row. -size can be -->
<!-- -small, -medium, -large, -xlarge (or none for default). these correspond to -->
<!-- max-width of 736px, 980px, 1280px, 1680px, respectively. -->
                    <div class="col-12">
                      
                      <a href="https://www.biorxiv.org/content/10.1101/708966v1">
                      
                      <h3>Nucleosome spacing heterogeneity drives chromatin elasticity</h3>
                      
                      </a>
                      
                    </div>
                    <div class="col-4 col-8-small">
                      <a
                      
                          href="files/poster-nuc-chain.pdf
"
                      
                      >
                      <img class="image fit" src="images/research-nuc-chain-poster.jpg"
                      alt="Image representing Nucleosome spacing heterogeneity drives chromatin elasticity">
                      </a>
                      
                      <p>(click to see full size)</p>
                      
                    </div>
                    <div class="col-8 col-12-small">
                      <p>It is now understood that chromatin's structure in the nucleus is more like disorganized "beads-on-a-spring" than the "30nm fiber" historically seen by electron microscopy. We analytically compute the Greens function for a diffusion in SO(3) subordinated to a CTRW (i.e. a wormlike chain with random, rigid kinks). This analytical models demonstrates that heterogeneity in nucleosome spacing drives structural disorder, and that the bulk properties of chromatin can be modulated by repositioning nucleosomes. More details available in the <a href="https://www.biorxiv.org/content/10.1101/708966v1">paper.</a></p>

                    </div>
                  </div>
                  
                  <hr />
                  <div class="row">
<!-- col-n(-size) makes a column that takes up n/12'ths of the row. -size can be -->
<!-- -small, -medium, -large, -xlarge (or none for default). these correspond to -->
<!-- max-width of 736px, 980px, 1280px, 1680px, respectively. -->
                    <div class="col-12">
                      
                      <a href="https://multi-locus-analysis.readthedocs.io/en/latest/finite_window.html">
                      
                      <h3>Beyond Meier-Kaplan: a statistical correction required when analyzing "lifetimes" in a finite observation window</h3>
                      
                      </a>
                      
                    </div>
                    <div class="col-4 col-8-small">
                      <a
                      
                          href="https://multi-locus-analysis.readthedocs.io/en/latest/finite_window.html"
                      
                      >
                      <img class="image fit" src="images/research-waiting-times.png"
                      alt="Image representing Beyond Meier-Kaplan: a statistical correction required when analyzing "lifetimes" in a finite observation window">
                      </a>
                      
                    </div>
                    <div class="col-8 col-12-small">
                      <p>Historically, the Meier-Kaplan correction has been used whenever an experiment
measure "lifetimes" that are "right-censored" (meaning you stop observing before
the event you're waiting for happens). However, not all right-censored waiting
times are correctly treated by the Meier-Kaplan approach. We have developed a
generalized framework for easily classifying right-censored waiting times, and a
simple, fast algorithm for generating correct survival curves (or histograms) of
real measured lifetimes. A Python package implementing these corrections can be
<a
href="https://multi-locus-analysis.readthedocs.io/en/latest/finite_window.html">
found here</a>.</p>

                    </div>
                  </div>
                  
                  <hr />
                  <div class="row">
<!-- col-n(-size) makes a column that takes up n/12'ths of the row. -size can be -->
<!-- -small, -medium, -large, -xlarge (or none for default). these correspond to -->
<!-- max-width of 736px, 980px, 1280px, 1680px, respectively. -->
                    <div class="col-12">
                      
                      <a href="https://elifesciences.org/articles/02758">
                      
                      <h3>A molecular heat engine for segregating and partitioning bacterial chromosomes</h3>
                      
                      </a>
                      
                    </div>
                    <div class="col-4 col-8-small">
                      <a
                      
                          href="https://elifesciences.org/articles/02758"
                      
                      >
                      <img class="image fit" src="images/research-parabs.jpg"
                      alt="Image representing A molecular heat engine for segregating and partitioning bacterial chromosomes">
                      </a>
                      
                    </div>
                    <div class="col-8 col-12-small">
                      <p>A <a href="https://elifesciences.org/articles/02758">coarse-grained model</a> of the ParABS plasmid partitioning system shows that <em>Caulobacter crescentus</em> can use it as a heat engine to drive segregation of the new copy of its chromosome before division. The general "burned-bridge" type heat-engine that we describe has <a href="https://www.pnas.org/content/pnas/113/46/E7268.full.pdf">now been shown</a> to also drive the canonical plasmid-partitioning behavior of ParABS in <em>E. coli</em>.</p>

                    </div>
                  </div>
                  
                  <hr />
                  <div class="row">
<!-- col-n(-size) makes a column that takes up n/12'ths of the row. -size can be -->
<!-- -small, -medium, -large, -xlarge (or none for default). these correspond to -->
<!-- max-width of 736px, 980px, 1280px, 1680px, respectively. -->
                    <div class="col-12">
                      
                      <a href="https://www.cell.com/cell/fulltext/S0092-8674(14)01499-8">
                      
                      <h3>A "constant extension"/"adder" model for bacterial cell size control</h3>
                      
                      </a>
                      
                    </div>
                    <div class="col-4 col-8-small">
                      <a
                      
                          href="https://www.cell.com/cell/fulltext/S0092-8674(14)01499-8"
                      
                      >
                      <img class="image fit" src="./images/cell-constant-extenstion-ga.jpg"
                      alt="Image representing A "constant extension"/"adder" model for bacterial cell size control">
                      </a>
                      
                    </div>
                    <div class="col-8 col-12-small">
                      <p>Analysis of high-throughput movies of <i>C. crescentus</i> and <i>E.
coli</i> growth were used to show that bacterial populations control their
average cell size not by timing how long the cells have lived, not by measuring
a specific cell size that triggers division, but via a random process where
each cell simply grows the same amount on average. The paper can be <a
href="https://www.cell.com/cell/fulltext/S0092-8674(14)01499-8">found here</a>.</p>

                    </div>
                  </div>
                  
                </section>
              </article>

            <!-- Writing -->
              <article id="writing" class="panel">
                <header>
                  <h2>Musings</h2>
                  <p>
    Almost everything that I would normally have made into a blog post is either
    a coding tutorial (see my <a href="#code"> code </a> page) or has now become
    a published paper (see my <a href="#science">science</a> page).
</p>
<p>
    However, if you are interested in the philosophy of science, perhaps you'll
    find something worthwhile here...
</p>


                  <span class="nav">
                    <a href="#science" class="jumplink pic">
                      <span class="arrow left icon solid fa-chevron-left"><span>See my scientifit work</span></span>
                    <a href="#contact" class="jumplink pic">
                      <span class="arrow right icon solid fa-chevron-right"><span>Go to my contact information</span></span>
                    </a>
                  </span>
                </header>

                
                <hr />

                <section>
                  <h3>What makes science...trustworthy?</h3>

<p>And why? <a href="https://www.pewresearch.org/science/2019/08/02/trust-and-mistrust-in-americans-views-of-scientific-experts/">Recent polls show</a>
that scientists are largely still trusted by the American public. However, dive
into conversation with anyone in the "mistrusts science" camp, and you'll
quickly see that the internet age has led to an erosion of trust in science that
would have been unfathomable to the generation that welcomed the atomic age.</p>

<p>While this eroded trust has serious societal consequences, I would challenge
scientists to restrain themselves from casting aspersions simply because
somebody answers "no" to a question like "do you, overall, trust the scientific
process to produce reliable conclusions?" After all, even among scientists, this
statement needs to be heavily qualified to be agreed with. Most scientists, in
private, would admit that the scientific process is only right "eventually", or
"most of the time" at best.</p>

<p>We hesitate to speak up about these nuances in public forums (or to aunt Betty on
Facebook) because we know that any concession that science can produce bad
results might be used as ammunition to not vaccinate children (or to justify
all of the money spent on healing crystals).</p>

<p>Instead, when people make <a href="https://youtu.be/0Rnq1NpHdmw">legitimate complaints</a>
about <a href="https://www.vox.com/2016/6/21/11962568/health-journalism-evidence-based-medicine">headline
whiplash</a>,
we tend to claim that those effects are just  due to reporting of "bad" science,
or "preliminary" results. We attribute the public's frustration to the
<a href="https://en.wikipedia.org/wiki/Replication_crisis">replication crisis</a>, or to
bad journalism. Or maybe we complain that the public simply does not know how to judge
their sources of information.</p>

<p>But we rarely let ourselves admit that the process of science is
imperfect, and that it often quite produces bad results.</p>

<p>We admonish these people for not trusting science, and in the next breath we
complain about how so many papers published in Nature, Science, and Cell are
full of just-plain-wrong results or conclusions. We complain that people don't
believe us when we say that our climate models will still have the same
predictions tomorrow, while being fully aware that they do not have <em>any</em> of the
resources required (nor the time) to actually verify that this is true to any
satisfying degree.</p>

<p>If you're a scientist right now, you might already be cringing even. Scared
that this blog post will stray too close to dangerous waters, where my words
might be misinterpreted by those seeking to denigrate science.</p>

<p>In this environment, it is easy for an outsider to see science as nothing but
another religion tyring to be sold to them. Science is the new scripture, and
scientists the new priests. We are meant to trust what they say, and where we
find inconsistency, we should understand that under further reflection, we would
find that no inconsistency actually exists. Every time we blaspheme or dare
challenge this new scripture, its priests disparage us and call us hethens, a
blight upon society. They force reconciliation or exile. And so, people come to
feel like they are not meant to understand science, but to simply believe it.</p>

<p>Clearly, if this is the impression that a person has received about science,
then there is a <em>fundamental problem</em> in how science has been communicated to
them.
<a href="https://journals.sagepub.com/doi/abs/10.1177/0093650214534967">Many</a>
<a href="https://www.pnas.org/content/111/Supplement_4/13664">people</a>
<a href="https://royalsocietypublishing.org/doi/10.1098/rsos.181870">have proposed</a> that
the solution is simply to more carefully and explicitly communicate uncertainty
in scientific results.
<!-- Unfortunately, we humans are famously awful at
understanding uncertainty. I'd go even further and say that the more we couch
uncertainty in the formal language of statistics recommended by the above
papers, the less likely the general public is to put in the effort to understand
it. --></p>

<p>But I think the problem runs deeper than uncertainty. It seems to me that, as a
community, we lack a cohesive language with which to talk about different
<em>kinds</em> of scientific results.
<!--
We may complain that the public is not
sufficiently acquainted with the process of science to make the nuanced
distinctions between, say, mechanistic results and well-established
phenomenological results...</p>

<p><em>But if this is true, it is our fault.</em></p>

<p>So what do I propose? I think scientists need to collectively admit that we're
not one monolithic discipline, and that we are not all to be trusted equally!
-->
You don't need to make a statistical statement to understand that building a
laser trap to test predictions about entangling electrons is different <em>in kind</em>
than measuring IQ and trying to make predictions about people's behavior (even
though the error bars might appear to be of similar size!). Our intuition that
these predictions have different levels of trustworthiness doesn't represent
different levels of uncertainty, it represents an understanding that these are
totally different types of activities.</p>

<p>And the dividing line is not between the social and "hard" sciences either!  The
hard sciences are chock full of purely statistical studies, such as GWAS
studies that map out every gene that correlates with this or that disease. While
genetics is a "hard" science, these results are not going to be as robust as
a double-blind, longitudinal study on the effects of various evidence-based
interventions designed to, for example, help traumatized children self-regulate
(a "properly" scientific investigation).</p>

<p>The scientific community
already internally uses various colloquialisms to classify different <em>types</em> of
scientific inquiry by what their "model space" looks like:</p>

<ul>
<li>"bottom-up" vs "top-down" modeling</li>
<li>"exploratory" vs "perturbatory" experiments</li>
<li>"mechanistic" vs "phenomenological" explanations</li>
<li>and many more...</li>
</ul>

<p>In what follows, I hope to present an accounting of this language, along
with recommendations on when each type of science is useful, and how much each
type should be trusted.</p>

<p>I hope that with this more nuanced language, we can <strong>fearlessly admit</strong> that
scientific results can be horribly, dangerously wrong, without
jeapordizing our
ability to speak authoritatively when it matters (climate change, vaccination,
etc). Because what matters isn't building people's allegiance to the some new
religion of science, it's giving them the language to understand how scientific
results should be interpreted, so that they can make their own judgements about
how this means they should act.</p>

<h3>Defining science</h3>

<p>You'd be hard-pressed to find a working scientist that doesn't have a personal
answer to this question. If they don't have it penned into words, they surely
have years of accumulated intuition that they can quickly use to decide whether
something is "good science", "bad science", or "not science at all" (depending
on how opinionated they are, they might even be willing to eschew the middle
category altogether!)</p>

<p>However, you'd be equally hard-pressed to find a conference or scientific forum
where two scientists aren't arguing about exactly whether a particular result
is or is not "good science". I have always felt that these conversations tend to
largely involve people talking right past each other. Sure, there are cases when
the entire room agrees that somebody has not done good science. However, it is
far more often the case that the real issue seems to be that different people
simply have different approaches to "science". After the references to
Popper and Hume are over, and the participants have stopped comparing their
personal scientific process to the development of the <a href="https://en.wikipedia.org/wiki/Standard_Model">Standard
Model</a>, it always seems that both
parties end up agreeing that everybody's approach is important (except for the
social scientists, they never get to be in the "in" group).</p>

<p>I whole-heartedly believe that this is because these conversations largely boil
down to people trying to pitch (or defend) their particular type of science as
being the "one true definition" of science. If people had the language to more
easily admit that the types of activities they are doing are as different as
night and day (often equally valuable) without having to defend them as living
up to some arbitrary standard of "true science", I think we'd all waste a lot
less time at conferences.</p>

<h3>A Model of Models</h3>

<p>In any case, it makes the most sense to follow my diatribe against people who
try to "define" what science is with a definition of science:</p>

<ul>
<li><em>Science is a set of strategies we use to build predictive models about the
world around us</em></li>
</ul>

<p>Hopefully this is a sufficiently abstract (read: useless) definition that nobody
can argue against it. It says nothing about what those strategies are, nor does
it even mention experimentation! However, "predictive model" is quite a loaded
word here, so it makes sense to unpack what is meant.</p>

<p>By "model", I tend to mean a mapping between observables and mathematical
objects. However, I don't mean that I require the "model" to be written down in
the conventional language of early 21st century mathematics, I merely want
whatever language is used to describe the observables to be unambiguous enough
to allow you to draw clear, logical conclusions about the observables. Some
examples:</p>

<ol>
<li>Kepler's "Law of Orbits": all planets move in elliptical orbits, with the
sun at one focus.
<ul>
<li>The observable (distance to the sun) is mapped onto a mathematical object
(the distance between an ellipse and one of its foci).</li>
<li>While not expressed in terms of algebraic equations, the language is
sufficiently clear that you can draw unambiguous logical conclusions, like
"the rate of change of the distance to the sun depends on both the speed
around the ellipse and where on the ellipse we are".</li>
</ul></li>
<li>The inverse square law of gravity: a much more classically "mathematical"
statement. Two objects \(m_1\) and \(m_2\) with positions \(r_1\) and \(r_2\) and masses
\(M_1\) and \(M_2\) will interact with a force of magnitude \(GM_1M_2/|r_1 - r_2|^2\).
<ul>
<li>The choice of observables and mathematical objects here can be done with
varying levels of sophistication, but as a simple case, we can take the
observables to be the positions and velocities of any set of masses (defined
as objects which we observe to travel as a unit when force is exerted on
them) and the mathematical object is a set of differential equations in
\(\mathbb{R}^3\) describing their time-evolution due to a force that acts on
\(m_i\) as \(\sum_{j\neq i} GM_iM_j(r_j - r_i)/|r_j - r_i|^3\).</li>
<li>By virtue of being a mathematical statement (a differential equation), the
unambiguity of the logical conclusions of this mapping are clear.</li>
</ul></li>
<li>Descriptive statements, e.g.: We have a sun.
<ul>
<li>The observable (existence of a sun) is trivially mapped onto the value
"True" in some 2-tuple ("True", "False").</li>
<li>This technically allows us to draw only the "trivial" conclusion, which is
the restatement of our observable. Nonetheless, this is the foundation upon
which more complicated models (which include multiple observables as input
and predict others as output) are built.</li>
</ul></li>
<li>Diffusion of gas molecules at equilibrium: a particle of gas at a given
temperature will have a mean squared displacement proportional to the square
root of the time elapsed.
<ul>
<li>Here, properly mapping between a mathematical object (a random variable)
that captures what is meant by the plain english statement and the gas
particle's position requires some sophisitication, but it can be done.</li>
<li>In particular, I just wanted to point out that models need not be
deterministic.</li>
</ul></li>
</ol>

<p>By "predictive", I mean something very subtle (because I don't think "causality"
is really important to science at all, more on that below, I promise I'm not
crazy). For a model to be "predictive", it merely needs to expresses a
relationship between different observable that we expect to <em>always be true</em>.
For example:</p>

<ol>
<li>Kepler's "Law of Orbits": once we determine two points on the ellipse, we can
predict what all of the values that are observable ("distances from the sun")
can take are.</li>
<li>Newtonian gravity: this is a parametric model, meaning it requires some
measurement of our observable to fully specify (in this case, we need to take
enough measurements of various bodies interacting gravitationally to measure
\(G\)). After initial parameterization, the model is predictive, it can take
any set of measured velocities and positions and (in theory) predict the
positions and velocities of the masses for all of time.
<ul>
<li><strong>NOTICE</strong>: I did not say "all of time in the future". We can just as well
predict what the positions should have been in the past as we can predict
what they will be in the future. We may not be able to retroactively test
this type of prediction (someone would have had to measure the past ahead
of time, hide the values from us, then give them to us later to compare our
predictions to), but it makes just as much sense to say that we've
"predicted" the past as it does to say we've "predicted" the future.  This
is what I meant when I said that I don't think "causality" is important to
science at all. Aliens in a universe where time can be traversed forwards
or backwards may have an easier time than us doing science, but they would
be doing science just the same.</li>
</ul></li>
<li>Descriptive statements fail to be predictive. They do not link multiple
observables, and so when interpreted as "models" they can only "predict"
themselves.</li>
<li>Diffusion of gas molecules at equilibrium: this is a simple parametric model
like Newton's laws above. The important thing to notice is that while the
prediction is statistical, it is expected to hold exactly in probability
(hence satisfying my "always true" requirement above).</li>
</ol>

<p>I guess it makes sense to explicitly compare my definition of "prediction" with
the common usage of the word "counterfactuals" within the philosophy of science.
In my opinion, this is an unnecessarily supercilious word.
Basically, a model is predictive if it can take some information about how the
world is and predict some other information about the world.
Once one has a predictive model, predictions can be made based on actual
observations, or else the inputs to the model can be drawn some other way, in
which case the resulting prediction is called a counterfactual.</p>

<h4>That's not science!</h4>

<p>"But wait!", you might be screaming. "\(X\) satisfies your definition, but I most
definitely would not call it science!"</p>

<p>I probably agree. But as I stated from the outset, I'm not in the business of
defining what science is and isn't. I'm much more interested in describing the
many things that people <em>call</em> science, so that we can have a language for
discussing their relative merits. And if \(X\) satisfies my definition, I bet
there's at least one person out there right now trying to pass it off as
science.</p>

<h3>Types of Science</h3>

<p>The meat and potatoes. I'll be updating these lists as people suggest new
entries to me, hopefully allowing me to apply the scientific process to my model
of what science is.</p>

<h4>Classifying science by goals</h4>

<p>The goals of people doing science vary wildly:</p>

<ol>
<li>Searching for trends
<ul>
<li>All exploratory research goes here, whether experimental or computational</li>
<li>Identifying a trend in observations is a necessary first step to
hypothesizing the existence of a more fundamental "natural law", and so
this goal is core component of modern "science".</li>
<li>"Machine learning" can largely be thought of as the partial automation of
this step.</li>
</ul></li>
<li>Discovering and testing "invariants"
<ul>
<li>A trend becomes an invariant whenever it is demonstrated to be robust
enough within some particular domain that there is no reasonable
expectation that a new experiment or replication would find it to now be
false within that domain.</li>
<li>An invariant is not sometimes true, it is <em>always</em> true.</li>
<li>For example: Newton's gravitational laws are an invariant within a
particular domain (sub-relativistic interactions).</li>
<li>Establishing the correct level of confidence that a trend is in fact an
invariant is one of the cornerstones of modern scientific "intuition".</li>
<li>Finding invariants is always the central goal of doing fundamental physics
research. I would go so far as to define "physics" to be exactly this type
of science (the type that searches for invariants).</li>
<li>Each new "universality" result in mathematics leads to a new kind of
"invariant" that we can understand, and use to link together more and
different kinds of observables.</li>
<li>When the community is collectively convinced that something is an
invariant, we call it a "natural law".</li>
<li>We will describe many ways in which we search for and test invariants
below, but it is worth pointing out here that it is typically useful to
not just directly probe the trend itself, but to instead probe its logical
consequences (within the framework of existing, established invariants) in
order to more thoroughly test its validity (e.g. searching for
gravitational waves to "test" whether Einstein's relativity describes and
invariant of space-time).</li>
</ul></li>
<li>Classifying emergent behavior
<ul>
<li>A predictive model (in the sense described above) is simply built by
selecting a specific set of "invariants". Once those invariants are chosen
as assumptions, they will often have arbitrarily complex (even
incomputable) logical consequences. Understanding these consequences is
one goal of "science", because without this understanding we never know if
we've discovered something "new" or something we "already knew" (i.e.
could have predicted with existing theory).</li>
<li>Often when exploring emergent behavior, trends will be shown to come from
existing (known) invariants, and instead of being promoted to invariants
themselves, will be better understood as consequences of existing
invariants.</li>
<li>Sometimes, even things which were previously taken to be invariants (such
as electricity and magnetism) will be shown to equivalently be emergent
behavior from a more simple invariant (in this case local U(1) symmetry).</li>
<li>Understanding the complex interplay between the many ways that known
invariants and potential new (candidate) invariants interact with each
other to create logical consequences (predictions), allows us to construct
increasingly comprehensive (and comprehensible) models of the diverse
observations that we encounter in our every day lives.</li>
<li>The coarse-graining of emergent behavior also allows us to make
robust predictions about the world around us that would typically require
more energy to compute than we have available, by allowing us to use
heuristics that come from "first-principles" (i.e. well-established
invariants).</li>
</ul></li>
</ol>

<p>In every scientific study I have ever participated in has been driven by the three goals above: The exploration of trends, classification into invariants, and the logical study
of how invariants interact to reveal novel emergent behavior.</p>

<h4>Classifying science by approach</h4>

<p>However, the methods and heuristics that I have learned to apply when reaching
for each of these goals are varied and overlapping, so it's worth listing them
independently (as opposed to trying to associate each type of "scientific
practice" with a particular "scientific goal"):</p>

<ol>
<li>Deriving logical implications of well-established invariants
<ul>
<li>These can be tested in order to verify the underlying assumptions
(invariants)</li>
</ul></li>
<li>Attempting to prove independence of particular sets of invariants
<ul>
<li>This is the goal of those trying to axiomatize quantum mechanics</li>
</ul></li>
<li>Deriving logical implications of trends that are potentially invariants
<ul>
<li>This is typically just called experimental design, but can be a science in
and of itself if the invariant set is large/complicated enough.</li>
<li>In order to be a scientist specializing in this process, you must have the
mathematical sophistication <em>and</em> sufficient knowledge of your system to
generate sufficiently non-trivial logical implications while making sure
they are also testable.</li>
</ul></li>
<li>Exploratory experimentation
<ul>
<li>Carrying out new tests to look for trends.</li>
<li>Clever design of exploratory experiments will ensure that the observables
measured span the largest possible space, while intersecting with
previously measured observables just enough to give confidence that the
techniques used are working.</li>
</ul></li>
<li>Perturbatory experimentation
<ul>
<li>This is basically the experimental version of #3. Often, it is impractical
to predict what the consequences of a particular set of known (or
proposed) invariants are, and so perturbatory experiments are used to
explore the space what would otherwise have been "counterfactuals".</li>
<li>Good perturbatory experimentation is reminiscent of good unit testing. You
want to hit all the edge cases, see how the different variables that
affect the system individual work before studying how they interact, etc.</li>
<li>Perturbatory experiments are often used to construct "phenomenological"
models, where no invariants are known, in order to allow us to ask
scientific questions at higher levels of abstraction.</li>
</ul></li>
</ol>

<h4>Model selection in practice</h4>

<p>Okay, so those are the goals of science and its methods, but recall that my
initial definition of science was</p>

<ul>
<li><em>Science is a set of strategies we use to build predictive models about the
world around us</em></li>
</ul>

<p>So largely what I've said above reduces to the statement that "predictive
models" are just "trends" that we are confident enough to label "invariants",
and that science is about uncovering these invariants and their consequences.</p>

<p>But that still leaves the most important part: what are the "strategies", in the
definition above? What are the ways in which we choose what sets of invariants
we care about, exclude those that are impossible or trivial, and construct the
best possible predictive model?</p>

<p><strong>Its just heuristics</strong>.</p>

<p>Thousands of years of heuristics that humans have either built into them, or
discovered (and lost, and rediscovered) about what types of evidence should be
considered trustworthy.</p>

<p>Here are some of the most well-known ones</p>

<ol>
<li>Trivial invariants are useless unless they parametrize more complex models
<ul>
<li>saying "the sun exists" is not science</li>
<li>measuring the force pulling two massive bodies together is useless
<em>unless</em> it's simply as a proxy to measure a parameter in a more broadly
predictive model like Newton's</li>
</ul></li>
<li>(Occam's razor) A model that can predict more from less assumptions is
typically better
<ul>
<li>At the end of the day, humans are supposed to understand science, so
simpler is better</li>
</ul></li>
<li>A model that cannot be differentiated from other valid models via an
actualizable observable is useless
<ul>
<li>this is the main counter-example to point 2 above, and the main complaint
about e.g. string theory</li>
<li>other examples of heuristic at play are distinguishing between, let's say,
a model of Newton's gravity and a model of Newton's gravity that contains
an unmeasurable, invisible horse in low earth orbit. While Occam's razor
is the more common thing to cite when discarding this kind of theory, the
fact that the invisible horse is unmeasurable is actually a bigger ding
than the added model complexity by far.</li>
</ul></li>
<li>(Occam's shaving cream) There exist many systems which are complicated enough
that a more complicated model should not be dismissed out of hand just because a
simple model explains some set of observations.
<ul>
<li>The set of all models that can explain existing observations is always
important to consider. This is especially true in biological and
soft-matter systems, i.e. in regimes where emergent, complex behavior
dominates our observables.</li>
</ul></li>
<li>(Anti-Hume) If something has been observed to be true enough times, then we
should just act as if it's always true in practice, lest we be frozen in
inaction.</li>
<li>(Hume's wisdom) Just because an invariant is well-established does not mean
it should not be challenged.
<ul>
<li>But maybe it should be challenged less often.</li>
</ul></li>
<li>(The blight of the social sciences) Models that are proposed in systems where
few invariants are known are less likely to stand up to the test of time.
<ul>
<li>A large fraction of the complaints leveled against the social sciences by
the "hard sciences" boil down to this idea: that there are no Maxwell's
equations of human behavior.</li>
<li>Said more formally, the less we know about a system, the larger that the
space of possible models can be (and concommitantly, the larger the space
of logically-sound predictions can be). So the less we know about a system
(i.e. the less we understand it mechanistically) the less we can trust
that a model that predicts something correctly is representing an actual
invariant of the system, as opposed to a coincidence or fluke.</li>
</ul></li>
</ol>

<h3>So what?</h3>

<p>So I've laid out some words that I like to use in conversation that describe the
various goals of different activities which are all called "science"
colloquially (and often, by the practitioners). But I have yet to make many
value judgements about which parts are the most important.</p>

<p>All parts are important.</p>

<p>I hope that by merely laying out a framework for discussion, I have managed to
avoid too much grandiosity. The entire scientific process is important, and
scientists at all levels should be able to say "I'm a X!" proudly, where X is
"data scientist" (searching for trends), "bottom-up modeling specialist" (e.g.
mathematician doing physics or basic research), "exploratory experimentalist"
(e.g. throwing money and sequencing at medical research problems and hoping
something sticks).</p>

<p>What matters, from my perspective, is that the statistical results of the data
scientist not be confused as having the same weight as fundamental results of
e.g. an experimental biochemist.</p>

<p>While these are both important pursuits, they are different pursuits.</p>

<p>And if they are both called "science", then the public is right to not blindly
trust all scientists, as different types will have different types of evidence
for the statements that they make, not just different levels of confidence.</p>

<p><!--
<!- - version 2 - -></p>

<p>Is it the case that in any universe with agents both complex and uniform enough
to derive the existence of computability, that there must be a certain degree of
universality (existence of fundamental natural laws) to allow these beings to
emerge?</p>

<p>In a sentence: science is set of strategies for learning about the world around
us by studying the logical implications of various properties of the universe
that we believe to hold universally.</p>

<p>This phrasing probably sounds silly. After all, if my conversations with
scientists are at all representative, anyone reading this will probably either
feel that the definition misses some important facet of what they think of as
"science", or else just think that the definition is uselessly abstract.
However, I think that (modulo semantic issues about what exact parts of the
broader "scientific process" we choose to call science) I can convince any
scientist that not only is this definition useful, but that it contains all the
"magic bits": the special ingredients that make science useful and successful.</p>

<p>Being a contrarian at heart, I think it's the most fun to start with
some examples of things that <em>look</em> like science, but that this definition
excludes as being strictly <em>not</em> science.</p>

<h4>Non-science #1: Statistics</h4>

<blockquote>
  <p>Say I am an intrepid young scientist, and having just learned about the
  "scientific method" in school, I am tasked with coming up with a science fair
  project. Since hypothetical me lives in the northeast United States, there are
  many abandoned buildings from the early 18th and 19th centuries. I have always
  wondered why the doorways and roofs seemed so short compared to those in my own</p>
</blockquote>

<p>One of the most
When is fitting, let's say, a linear model, considered science? It's easiest to
see by example. When measuring the velocity of something falling on the moon, it
seems to make scientific sense to fit this (possibly noisy) data to a linear
model. This is because we have hundreds of years of data suggesting that an
approximately constant force of gravity will be the only real contributor to the
motion, and so we expect that if we measured everything more accurately
(decreasing the noise) that the linear model would continue to describe the data
well.</p>

<p>On the other hand, fitting estimates for the average height of all humans in
each year with a linear function seems less "scientific".  Even if the data
looks basically like a noise linear process, we do not expect at all that if we
measured more and more people (or more and more years in the past/future) that
the linear model would continue to perform well. For a model to be a "scientific
model", we must <em>reasonably expect</em>that the model will continue to work
as we collect more measurements.</p>

<p>Don't get me wrong, statistics is incredibly important to the scientific
process.</p>

<blockquote>
  <p>Fits from statistical models can even be used as inputs (assumptions) for more
  scientific models (if we simply show that the consequences of the scientific
  model we care about don't depend strongly on the exactly output of the
  statistical model).
  But the slope of a linear model that was fit to a cloud of data should not be
  called a "scientific" finding. It is (merely) a statistical finding.</p>
</blockquote>

<h4>Non-science #2: Unverifiable models</h4>

<p>Models that can only be distinguished from other models via predictions that can
not be verified in any way are not scientific.</p>

<p>This most famously includes string theory, but also encompasses as a special
case certain types of model selection that are normally relegated to Occam's
razor.  For example, when comparing between a model of gravity and a model where
gravity has identically the same properties but is generated by an unmeasurable
space fairy that orbits outside the milky way, the latter can be dismissed out
of hand as being unscientific. If we replace the space fairy with a massive
obelisk in low-earth orbit, now the latter theory is at least scientific (it is
testable) although it is true that heuristically, it makes sense to dismiss it
out of hand (this is Occam's razor), the reasons we dismiss it are totally
different, even if they seem similar.</p>

<h4>Defining science by the "scientific method"</h4>

<p>While (arguably) useful for teaching children about what doing science is like,
the scientific method is far enough removed from the everyday experience of
doing science that, as I near the end of my PhD, it seems comical to even write
it down.</p>

<ol>
<li>Ask a question</li>
<li>Do background research</li>
<li>Construct a hypothesis</li>
<li>Test with an experiment</li>
<li>Analyze the data
<ul>
<li>if the results align with the hypothesis, you can't reject it</li>
<li>if the results <em>don't</em> align with the hypothesis, you can reject it!</li>
</ul></li>
<li>Communicate your results (and have them replicated and vetted).</li>
</ol>

<p>Most scientists will feel that this process does vaguely reflect the steps they
follow. Maybe it doesn't reflect their personal practice of science, but it
definitely does describe the outline of the story they have to tell at the end
of the day when drafting a paper or writing a grant.</p>

<p>What this definition lacks is any tool for describing the line that separates
exploratory statistical analysis and science. What, after all, is the difference
between what humans do when we look at the world around us and what a modern
machine learning algorithm does?</p>

<p>Modern ML has become much better than humans at uncovering hidden statistical
correlations between the various observations it makes of its surroundings.
However, the point of science is to identify which of these
correlations should be thought of</p>

<p>But what this description is missing is the most important part of the
scientific method: how to formulate a hypothesis for it to be scientific.</p>

<p>As implied by step 2 above, science is not done in a vacuum. There are many
scientific "facts" which we accept with various degrees of confidence.</p>

<h4>Defining science by process</h4>

<p>The process of science is often summarized by a simple loop:</p>

<ol>
<li>Identify some property of the world around us that seems to be universally
true.</li>
<li>Identify the logical consequences of that property.</li>
<li>Test for the logical consequences, and keep poking around until you're either
unable to falsify your theory given your operating constraints or until
you've found some inconsistency.</li>
<li>When an inconsistency is found, return to step 1 and identify the new
universal property.
<!--
-->
<!-- Of course, science often engages in some -->
<!-- seriously meta games. It might be that -->
<!-- instead of observing first some symmetry of -->
<!-- the universe, we instead first observe a -->
<!-- bunch of particles in our collider that -->
<!-- we know mathematically would emerge in the -->
<!-- case when there is some underlying symmetry -->
<!-- (which may actually predict many other -->
<!-- particles that we have not seen before). -->
<!-- This doesn't really follow the 4-step plan I -->
<!-- lay out above (even though it is obviously -->
<!-- an example of extremely good science). -->
<!-- However, it does follow the gestalt of the -->
<!-- above, in that it involves identifying a -->
<!-- specific set  of mathematical (i.e. -->
<!-- axiomatizable) "assumptions" whose logical -->
<!-- consequences can be directly tested. -->
<!--  -->
<h4>Defining science as "model-driven"</h4></li>
</ol>

<p>The cornerstone of the process described above is what I'd call the "successful,
falsifiable model".</p>

<ul>
<li>By <em>"model"</em>, I mean a mathematical description of some process that
generates observations of the world around us. (Where mathematical simply
means it is unambiguous, and has unambiguous logical consequences).</li>
<li>By <em>"successful"</em>, I mean that we can be reasonably confident that the
model will remain accurate upon continued testing.</li>
<li>Finally, by <em>"falsifiable"</em>, I mean that every "assumption" of the
model can in principle be tested.</li>
</ul>

<p>You might notice in particular that I've punted hundreds of years of philosophy
of science into the catch-all phrase: "reasonably confident".  This is not
because I don't understand the complexity involved in trying to specify exactly
what are the properties that should make one "reasonably confident" that a model
will remain accurate upon adding further measurements. Rather, it is exactly
because I admit that I personally believe that "good" science (as currently
practiced) is basically just a huge grab bag of heuristics for what kinds of
models are useful (e.g.  those that have stood the test of time and repeated
verification, those that are as simple as possible, those that rely on the
fewest unmeasured parameters, etc). And while I think these are important for
doing "good" science, in practice, they are ignored by working scientists as
needed to accomplish their particular goals.</p>

<p>In particular, notice that I don't include any notion of the "simplicity" of the
model in my definition of science. That is in fact because I actively do not
think that science is necessarily about the pursuit of the "simplest possible"
model of a system. In fact, I tend to actually <a
href="#blog.physics-vs-biology">define physics</a> as the subset of science
whose sole purpose is to generate the most simple model of a system possible
given existing data.</p>

<h4>Defining science as leveraging logic to facilitate the search for truth</h4>

<p>Science turns out to be a word which is used to describe quite a diverse range
of activities that are undertaken in the search of knowledge.
1. (exploratory experimentation and hypothesis development) Broadly, searching
   for underlying universal properties of the world around us by looking for
    patterns in observations.
2. (theoretical development) Developing the logical consequences of existing
   ("known") patterns in the world around us.
3. (experimental verification and falsification) "known" consequences of "known"
   patterns in the world around us to look for discrepancies, since these will
   stem (in a real model) from incorrect assumptions that should be revised.</p>

<blockquote>
  <p>By "universal properties" above, I don't necessarily mean "fundamental" or
  "simplest", just "reproducible". Even without a complete understanding of
  epigenetics, we were able to map out how gene expression works for some easy
  bacterial genes early on (e.g. the <i>lac</i> operon). The principle that a
  transcription factor needs to be chemically modified by the environment and bind
  to the promoter was not a "universal" or "fundamental" description of how gene
  expression works, but it was the best description that we had (and agreed with
  all data available at the level of coarse-graining that was required by models
  of the time).</p>
</blockquote>

<p>However, if we simply go around calling everything that seems to fit in this
extremely broad framework by the name "science", we will find that it does not
distinguish very well between well-respected theories and total quack science.</p>

<h4>Things that are science</h4>

<p>Many theoreticians would probably take offense to my requirement that scientific
models need to be expected to be able to reproduce experimental data arbitrarily
well. After all, many models (my entire scientific career included) are
considered worthwhile not because they can reproduce experimental measurements.
For example, a very popular approach widely known as "bottom-up" modeling
totally eschews attempting to match any data at all.  Instead, bottom-up
modeling takes small subset of only the most well-accepted facts about a complex
system and tries to derive the consequences of only that subset of properties.
By doing so, the modeling process is made infinitely more tractable, and the
individual effects from the different parts of the system can often be teased
apart much more easily. However, this of course comes at the cost of being able
to reproduce data.</p>

<h4>Assumption complexity: a useful classifier for "types" of science</h4>

<p>I find that one of the most useful metrics by which to classify scientific
models is by how detailed the assumptions need to be in order to produce the
model's logical conclusions (given our current understanding of mathematics).</p>

<p>On one extreme, you'll tend to find models that come out of very applied fields
such as biology or materials science. Say you are interested in the science
behind how a microbe works. It's entirely possible that in order to develop a
model that faithfully reproduces every facet of such a complicated entity, you
need to basically plug in so many details (assumptions) about the microbe in
question that you begin to ask yourself if you're actually learning anything or
just carefully reverse-engineering. These models gain their utility from being
able to explain specific systems that affect humans here and now.</p>

<p>On the other extreme, you'll tend to find models from more "theoretical" fields
such as physics or computer science.  Practitioners of these fields will tend to
specifically be interested in what logical conclusions can be drawn from small
sets of assumptions. For example, a physicist might ask what assumptions can be
drawn about the electromagnetic force assuming a particular type of symmetry
(locally U(1)) that it has been observed to obey. These models will contain a
very small set of assumptions and as provide utility by being broadly applicable
to a wide range of problems.</p>

<p>Both types (and everything in between) are useful. Both types are science.</p>

<p>Are there things which are "not" science? Definitely.</p>

<p><!- - first version - -></p>

<p>Any attempts to build a falsifiable model that describes observations about the
world around us is science, as is the testing of such a model, as is the
exploration of potentially unexplored types of measurements about the world with
the purpose of building a model. At least that's a tempting thing to say, and
maybe it summarizes the most well-accepted bits of the "philosophy of science",
from Bacon, to Popper to Hempel.</p>

<blockquote>
  <p>However, I often see this definition
  being used as a sort of checklist, by people
  that want to do "science", but somehow end
  up doing bad stats on ugly data.</p>
  
  <p><strong>BUT</strong>, I think that almost any modern scientist, if forced to,
  could do a much better job at describing what is meant at the turn of the 21st
  century by "good" science than this simple description.
  It's easy to do better than centuries of
  brilliant philosophers at describing
  something that you have centuries more
  examples of, but we're going to do it
  anyway.</p>
</blockquote>

<p>First, what is so bad about the previous definition? When used as a checklist,
it allows one to call a lot of things science that a working scientist would
take great offense to. For example, say I measure a bunch of people's heights
over the last few decades. Now I fit a line to their heights as a function of
time. I claim that people's heights are growing at an average rate given by the
parameter of my model fit. The number of ways in which this is "hardly" science
are almost too numerous for any working scientist to point out, but it seems to
check all of the boxes! There's a model (albeit an unmotivated, oversimplified,
seemingly arbitrary one). It's falsifiable (more measurements, or maybe
measurements in the future or past could show that this trend no longer holds or
never held at all).  My measurements might have been the first of their kind, I
might have predicted height would increase on average over time <i>a priori</i>,
and still no scientist worth their salt would call what I did science.  Maybe if
confronted with a definition of science like the one above, they would cave and
just mutter that it's not "good" science in any case. Many of the less
politically correct...physicists...in the room would even be likely to mutter
that "this might pass for real work in the social sciences, but not in [my]
department."</p>

<p>Now I don't agree with my colleagues that think that social sciences are
"lesser" than the "hard" sciences, but the things they are recoiling at stem
from a shared understanding of what types of science are useful, what types of
scientific questions tend to be fruitful, and what types of approaches tend to
most often lead to bogus results. The above example really does go against
dozens of unspoken "rules" about what is allowed to be called science within the
community. They recoil because such a seemingly useless piece of work flows
directly against the zeitgeist of early 21st century science, and this zeitgeist
exists for a reason.</p>

<p>Well, the obvious follow-up question is: can we amend our previous definition of
science to more completely contain this shared understanding of what constitutes
"real" science, as opposed to things that simply masquerade as scientific? Of
course, as someone whose graduate "schooling" was largely <a
href="#blog.physics-vs-biology"> guided by physicists</a>, it is my solemn duty
to believe that not only can I summarize this complex set of rules, but that I
can do so in a manner that distills them into a much simpler set of underlying
principles that should drive how everybody does science.</p>

<p>Jokes aside, dozens of much smarter people than me have tried this same exercise
over the centuries since humans have left behind writings about science.
However, like a middle schooler solving algebraic equations that only
specialized mathematicians would have been acquainted with in ancient times, I
have the advantage of modernity: everything is much easier in hindsight.</p>

<p>In order to better define what science is, we're going to sketch out a model for
how an abstract (set of) agent(s) might go about discovering things about the
world around themselves. I don't intend to provide a rigorous definition or
example of such a model, but merely to point out the types of strategies that
might emerge, so that I can more precisely specify which of these I would call
"science".</p>

<p>So if you want a model describing a set of agents trying to discover the
universe around them, you need to specify something about the universe that
you're talking about. And if you want to say things about how universally
applicable their strategy is, you need to in fact describe the sets of universes
over which you want to gauge their potential for success.</p>

<p>This may seem like a silly proposition, but I'd argue that we can quickly list
off various properties of universes that are necessary for the question we're
asking to even make sense in the first place.</p>

<ol>
<li>The universe better be complex enough that it can support (as in it can
contain) agents that can ask questions about the universe.</li>
<li>On the time scales on which these agents ask their question, the universe
better have at least reasonably constant underlying rules.</li>
</ol>

<p>The complexity lower-bound means that the universes we're talking about have to
at least admit Turing-complete machines.  The other property is mostly just to
simplify the questions that we're asking.  And while there are <a
href="https://en.wikipedia.org/wiki/Time-variation_of_fundamental_constants">
valid reasons to ask if the laws of physics as-currently-written describe rules
about our own universe that are actually "unchanging"</a>, the majority of the
scientific community can safely assume that the fundamental laws of physics are
unchanging (hence the "reasonably" above).</p>

<p>For our model of the "scientific" process, we will consider a observable
universe, a set of existing observations, and a set of (possibly evolving)
methods for generating new observations. A "knowledge set" contains a set of
patterns that have been observed to hold (exactly, statistically, or otherwise)
in the existing observations.</p>

<p>We define "reason" to be the set of rules by which the elements of the
"knowledge set" are assigned "confidences". And we define "science" to be the
process by which new patterns in existing observations are searched for
(hypothesis development), their consequences are enumerated (theoretical
development, i.e. mathematics), and new measurements are chosen with which to
attempt to verify or falsify existing hypotheses.</p>

<p>Is reason just really fancy statistical pattern-matching with some kind of
power-law memory decay? Yeah probably.</p>

<p>In this framework, some "good" types of science are:</p>

<ul>
<li>Finding</li>
<li>Finding consequences of high-confidence hypotheses that directly contradict
low-consequence hypotheses, and removing the contradictory low-confidence
hypothesis.</li>
<li>Finding consequences of high-confidence hypotheses that are also in our
knowledge set. Patterns in our knowledge set that are</li>
</ul>

<p>--></p>

                </section>
                
                <hr />

                <section>
                  <h3>When Eric Weinstein and Peter Thiel get together...</h3>

<p>...you're sure to hear grandiose claims about the stagnation of science and no
real substance to back it up. Or at least that's what you'll get if you watch
Eric's recently released <a href="https://www.youtube.com/watch?v=nM9f0W2KD5s">
first podcast episode</a> (I would recommend against).</p>

<p>Potshots aside though, I was pretty flabbergasted by their claim that since the
early 1970s, science as a whole has more or less frozen in time, and that
academic progress has ground to what amounts to a complete halt. Perhaps I am
blinded by young naiveté, but from my perspective several fields (especially
biology, which they seem to have it out for in particular) have done nothing but
accelerate the pace of their development over the past ten years. From the human
genome project to CRISPr, it's hard to deny that advances in our ability to
probe and modify our own biology is advancing at an exponential rate even faster
than Moore's law.  (And this comes just as Moore's law itself is slowing down
for the first time since the 70s, the point of time at which Peter and Eric
claim the great demise of science began).</p>

<p>So why the pessimism on their part? Well they really do harp on the idea that
progress in biology, (after molecular biology was "founded by physicists") never
progressed at a pace comparable to that of our understanding of "the atom". They
make sure to never pin down exactly what would constitute a "fast" pace of
biological discovery, but they make sure to explicitly posit that biology full
of "not [as] talented people" as physics.</p>

<p>If you spend any non-zero amount of time at the interface of math, physics, and
biology, this is a pattern that you'll see come up over and over again.
Physicists (or in Thiel's case, simply self-proclaimed "intellectuals"),
always think that with their superior creativity/intelligence/mathematical
ability, they can swoop into biology, arguably the most populated area of modern
research, and swiftly lay down some kind of simple foundational principles that
have managed to elude biologists for the past century.</p>

<p>Invariably, some fraction of these physicists actually do start doing biological
research, and quickly find themselves drowning in a sea of protein names,
apparently useless acronyms for genes, and systems that require much more
specification and have many fewer "universal" properties than they would have
anticipated. Most run away before getting very far, but many make non-trivial
contributions (after all, its true that typical biologists tend to prefer having
other people do their math/modeling for them, so that they have more time to do
experiments). That said, only a very few become "card-carrying" biologists, able
to hold their own in a conversation with the mythical "real" biologist, whose
encyclopaedic knowledge of historical experiments, networks of genes, and
post-translational modifications related to their system of interest <em>literally</em>
dwarfs even the literature itself, due to years of hearing folkloric,
unpublished results through the grapevine.</p>

<p>What separates these "card-carrying" biologists from the physicists that run
away? Is it as Eric and Peter would probably suggest, that they have simply
become focused on less interesting questions? Have they simply lost their love
or ability to do mathematics?</p>

<p>I think what separates the wheat from the chaff is the realization that the
fundamental principles of biology have long been well known, just as they have
been for physics. Namely:</p>

<ol>
<li>Physics still works inside living cells. The laws of physics still lie at the
heart of how cells, tissues and organisms work.  Supposing God himself had
gifted you with a list of every relevant molecule and interaction in a
biological system, you won't meet many biologists who will claim that an
appropriate computer simulation wouldn't be able to reproduce experimental
results as well as can be done in fundamental physics research.</li>
<li>Evolution by natural selection has, broadly speaking, been in charge of the
development of all modern forms of life. Therefore, all control systems
within living organisms must have developed through random mutation. As a
consequence, there are an absolutely massive number of leftover and/or
partially-functioning systems in any given organism. You can't search for a
protein by knowing what the optimal way for the cell to solve a problem would
be (although biology does optimize some things pretty well) because natural
selection need not find the best solution, it just finds <em>some</em> solution,
then "tries" to make it better over time.</li>
</ol>

<p>What the physicists-turned-biologists that I respect will all universally admit
is that biology seems to follow what I like to call the "principle of most
action": if you think you know the answer to how something is being done by a
cell, you are probably missing 10 more key players, several dozen more backup
systems, and the part that you think is most important may even be just
vestigial.</p>

<p>Is this status quo (that most biological "models" are doomed to fail) just a
side effect of biologists not being smart enough to make good models? NO!
It's built right into the thing that's being studied. Organisms that spent
billions of years stochastically searching DNA sequence space for genomes that
would keep them around for evolutionary time are just going to be complex. The
fact that biologists tend to be skeptical of the physicist's approach of
building "simple" models to gain understanding in a "bottom-up" approach is well
founded, because they have decades of experience as a field finding that the
simplest explanation just wasn't the one that stood up to more experiments.</p>

<p>If you've made it this far somehow, you might be wondering why I'm writing this?
I guess it upsets me that so many of Eric and Peter's valid criticisms of
academia (the fact that tenure is a pyramid scheme, the perverse incentive
structures, etc) are mixed with these grandiose ideas about how much smarter
they must be than people in these other fields, since those fields are "moving
so slow".</p>

<p>Hopefully someday I can come back and rewrite this into a guide: "how to be a
good biologist for <span style="text-decoration: line-through;">dummies</span>
physicists".  But for now I guess it will remain a rant about the pitfalls of
pontificating about a field you know nothing about.</p>

                </section>
                

              </article>

            <!-- Contact -->
              <article id="contact" class="panel">
                <header>
                  <h2>Contact Me</h2>
                  <span class="nav">
                  <a href="#writing" class="jumplink pic">
                    <span class="arrow left icon solid fa-chevron-left"><span>Read some of my thoughts</span></span></a>
                  </span>
                </header>
                <form action="mailto:brunobeltran+com@gmail.com" method="get">
                  <div>
                    <div class="row">
                      <div class="col-6 col-12-medium">
                        <input type="text" name="name" placeholder="Name" />
                      </div>
                      <div class="col-6 col-12-medium">
                        <input type="text" name="email" placeholder="Email" />
                      </div>
                      <div class="col-12">
                        <input type="text" name="subject" placeholder="Subject" />
                      </div>
                      <div class="col-12">
                        <textarea name="body" placeholder="Message" rows="6"></textarea>
                      </div>
                      <div class="col-12">
                        <input type="submit" value="Send Message" />
                      </div>
                    </div>
                  </div>
                </form>
              </article>

          </div>

        <!-- Footer -->
          <div id="footer">
            <ul class="copyright">
              <li>&copy; Bruno Beltran.</li><li>Design inspired by: <a href="http://html5up.net">HTML5 UP</a></li>
            </ul>
          </div>

      </div>

    <!-- Scripts -->
      <script src="assets/js/jquery.min.js"></script>
      <script src="assets/js/browser.min.js"></script>
      <script src="assets/js/breakpoints.min.js"></script>
      <script src="assets/js/util.js"></script>
      <script src="assets/js/main.js"></script>

  </body>
</html>